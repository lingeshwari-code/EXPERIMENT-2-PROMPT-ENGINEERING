# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.

## Algorithm:
Step 1: Identify the prompting patterns to be analyzed, namely Zero-Shot, Few-Shot, Chain-of-Thought, Role Prompting, Instruction Prompting, and Self-Consistency.

Step 2: Select diverse test scenarios to cover different task categories such as text summarization, translation, mathematical problem-solving, creative writing, professional communication, and coding or debugging.

Step 3: For each scenario, design two types of prompts: a broad or unstructured prompt (e.g., “Summarize this text”) and a refined or structured prompt (e.g., “Summarize this text in exactly three sentences highlighting only the main points”).

Step 4: Apply each prompting pattern to both types of prompts in every scenario and record the generated responses systematically.

Step 5: Evaluate the outputs based on three key metrics—quality (fluency, grammar, coherence), accuracy (factual correctness or logical validity), and depth (completeness and detail of response). Use a numerical scoring scale such as 1 to 5 for consistency.

Step 6: Organize the evaluation results into a comparative results table for each scenario, listing scores for all prompting patterns and prompt types.

Step 7: Analyze the results to identify which prompting patterns perform best under different conditions—for instance, simple tasks, reasoning-heavy tasks, creative writing, or professional communication.

Step 8: Conclude the experiment by summarizing the strengths and weaknesses of each prompting pattern and recommending the most suitable approaches for specific applications.

## Output
[exp-2 promt.pdf](https://github.com/user-attachments/files/22081271/exp-2.promt.pdf)
## Result
The experiment was conducted across six scenarios (text summarization, translation, mathematical problem-solving, creative writing, professional communication, and coding/debugging) using six prompting patterns (Zero-Shot, Few-Shot, Chain-of-Thought, Role Prompting, Instruction Prompting, and Self-Consistency). Each prompt was tested in both broad/unstructured and refined/clear forms.
